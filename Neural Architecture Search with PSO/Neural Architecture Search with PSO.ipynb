{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom keras.layers import Input, UpSampling2D\nfrom keras.models import Model\nfrom tensorflow.keras.datasets import cifar100\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, AveragePooling2D, GlobalAveragePooling2D, BatchNormalization, Activation, Dropout, Concatenate, Reshape\nfrom tensorflow.keras.optimizers import SGD, Adam","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:03.346255Z","iopub.execute_input":"2025-01-15T18:10:03.346566Z","iopub.status.idle":"2025-01-15T18:10:11.647593Z","shell.execute_reply.started":"2025-01-15T18:10:03.346530Z","shell.execute_reply":"2025-01-15T18:10:11.646876Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"(x_train, y_train), (x_test, y_test) = cifar100.load_data()\nnum_classes = 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:11.648418Z","iopub.execute_input":"2025-01-15T18:10:11.648888Z","iopub.status.idle":"2025-01-15T18:10:16.511683Z","shell.execute_reply.started":"2025-01-15T18:10:11.648865Z","shell.execute_reply":"2025-01-15T18:10:16.510706Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"x_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:16.512777Z","iopub.execute_input":"2025-01-15T18:10:16.513114Z","iopub.status.idle":"2025-01-15T18:10:16.779911Z","shell.execute_reply.started":"2025-01-15T18:10:16.513085Z","shell.execute_reply":"2025-01-15T18:10:16.779211Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"y_train = tf.keras.utils.to_categorical(y_train, num_classes)\ny_test = tf.keras.utils.to_categorical(y_test, num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:16.780648Z","iopub.execute_input":"2025-01-15T18:10:16.780854Z","iopub.status.idle":"2025-01-15T18:10:16.817885Z","shell.execute_reply.started":"2025-01-15T18:10:16.780836Z","shell.execute_reply":"2025-01-15T18:10:16.817222Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"input_dim = 20\nlatent_dim = 8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:16.818784Z","iopub.execute_input":"2025-01-15T18:10:16.819102Z","iopub.status.idle":"2025-01-15T18:10:16.822952Z","shell.execute_reply.started":"2025-01-15T18:10:16.819072Z","shell.execute_reply":"2025-01-15T18:10:16.822120Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def generate_block_vectors(lmin, lmax, gmin, gmax, n):\n    block_vectors = []\n    for _ in range(n):\n        n_layers = np.random.randint(lmin, lmax + 1)\n        block_vector = [np.random.randint(gmin, gmax + 1) for _ in range(n_layers)]\n        block_vector += [0] * (lmax - n_layers)  # Pad with zeros\n        block_vectors.append(block_vector)\n    return np.array(block_vectors)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:16.825517Z","iopub.execute_input":"2025-01-15T18:10:16.825754Z","iopub.status.idle":"2025-01-15T18:10:16.835701Z","shell.execute_reply.started":"2025-01-15T18:10:16.825734Z","shell.execute_reply":"2025-01-15T18:10:16.835006Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"lmin, lmax = 10, 20\ngmin, gmax = 10, 32\nn_samples =  15000\nbatch_size = 64","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:16.837296Z","iopub.execute_input":"2025-01-15T18:10:16.837512Z","iopub.status.idle":"2025-01-15T18:10:16.846721Z","shell.execute_reply.started":"2025-01-15T18:10:16.837490Z","shell.execute_reply":"2025-01-15T18:10:16.845993Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Normalize block vectors\ndef normalize_block_vectors(block_vectors, gmin, gmax):\n    return (block_vectors - gmin) / (gmax - gmin)\n\n# Denormalize block vectors\ndef denormalize_block_vectors(block_vectors, gmin, gmax):\n    return np.round(block_vectors * (gmax - gmin) + gmin)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:16.847457Z","iopub.execute_input":"2025-01-15T18:10:16.847750Z","iopub.status.idle":"2025-01-15T18:10:16.860379Z","shell.execute_reply.started":"2025-01-15T18:10:16.847728Z","shell.execute_reply":"2025-01-15T18:10:16.859740Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"block_vectors = generate_block_vectors(lmin, lmax, gmin, gmax, n_samples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:16.861251Z","iopub.execute_input":"2025-01-15T18:10:16.861486Z","iopub.status.idle":"2025-01-15T18:10:17.482965Z","shell.execute_reply.started":"2025-01-15T18:10:16.861458Z","shell.execute_reply":"2025-01-15T18:10:17.482184Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"block_vectors[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:17.483697Z","iopub.execute_input":"2025-01-15T18:10:17.483933Z","iopub.status.idle":"2025-01-15T18:10:17.489783Z","shell.execute_reply.started":"2025-01-15T18:10:17.483910Z","shell.execute_reply":"2025-01-15T18:10:17.488996Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"array([10, 24, 23, 25, 32, 11, 28, 23, 20, 29, 13, 30, 25, 16, 17, 25, 29,\n       32, 19, 11])"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def create_pairwise_datasets(block_vectors, batch_size):\n    n_samples = block_vectors.shape[0]\n    indices1 = np.random.choice(n_samples, batch_size, replace=False)\n    indices2 = np.random.choice(n_samples, batch_size, replace=False)\n    input1 = block_vectors[indices1]\n    input2 = block_vectors[indices2]\n    return input1, input2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:17.490611Z","iopub.execute_input":"2025-01-15T18:10:17.490984Z","iopub.status.idle":"2025-01-15T18:10:17.502847Z","shell.execute_reply.started":"2025-01-15T18:10:17.490944Z","shell.execute_reply":"2025-01-15T18:10:17.502212Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class LatentNormalizationLayer(tf.keras.layers.Layer):\n    def __init__(self, epsilon=1e-8, scale_range=(0, 1), **kwargs):\n        super(LatentNormalizationLayer, self).__init__(**kwargs)\n        self.epsilon = epsilon\n        self.min_val, self.max_val = scale_range\n\n    def call(self, inputs):\n        # Normalize to have zero mean and unit variance\n        mean = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n        std_dev = tf.math.reduce_std(inputs, axis=-1, keepdims=True) + self.epsilon\n        normalized = (inputs - mean) / std_dev\n        \n        # Scale to the specified range (e.g., [0, 1])\n        scaled = 0.5 * (normalized + 1)  # Scale to range [0, 1]\n        scaled = tf.clip_by_value(scaled, self.min_val, self.max_val)  # Clip to avoid out-of-range values\n        return scaled\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:17.503678Z","iopub.execute_input":"2025-01-15T18:10:17.504212Z","iopub.status.idle":"2025-01-15T18:10:17.516156Z","shell.execute_reply.started":"2025-01-15T18:10:17.504185Z","shell.execute_reply":"2025-01-15T18:10:17.515407Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def create_encoder(lmax, latent_dim):\n    inputs = tf.keras.Input(shape=(lmax,))\n    x = tf.keras.layers.Dense(256, activation=\"relu\")(inputs)\n    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n    outputs = tf.keras.layers.Dense(latent_dim, activation=\"sigmoid\")(x)  # Values in [0, 1]\n    encoder = tf.keras.Model(inputs, outputs, name=\"encoder\")\n    return encoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:17.516969Z","iopub.execute_input":"2025-01-15T18:10:17.517216Z","iopub.status.idle":"2025-01-15T18:10:17.529535Z","shell.execute_reply.started":"2025-01-15T18:10:17.517184Z","shell.execute_reply":"2025-01-15T18:10:17.528773Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def create_decoder(latent_dim, lmax, gmin, gmax):\n    inputs = tf.keras.Input(shape=(latent_dim,))\n    x = tf.keras.layers.Dense(128, activation=\"relu\")(inputs)\n    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n    x = tf.keras.layers.Dense(lmax, activation=\"sigmoid\")(x)  # Output normalized to [0, 1]\n    \n    # Denormalize and round to integers\n    outputs = tf.keras.layers.Lambda(lambda x: tf.round(x * (gmax - gmin) + gmin))(x)\n    \n    decoder = tf.keras.Model(inputs, outputs, name=\"decoder\")\n    return decoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:17.530251Z","iopub.execute_input":"2025-01-15T18:10:17.530444Z","iopub.status.idle":"2025-01-15T18:10:17.542334Z","shell.execute_reply.started":"2025-01-15T18:10:17.530427Z","shell.execute_reply":"2025-01-15T18:10:17.541428Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"encoder = create_encoder(lmax, latent_dim)\ndecoder = create_decoder(latent_dim, lmax, gmin, gmax)\n\ninputs = tf.keras.Input(shape=(lmax,))\nencoded = encoder(inputs)\ndecoded = decoder(encoded)\nautoencoder = tf.keras.Model(inputs, decoded, name=\"autoencoder\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:17.543150Z","iopub.execute_input":"2025-01-15T18:10:17.543436Z","iopub.status.idle":"2025-01-15T18:10:18.395047Z","shell.execute_reply.started":"2025-01-15T18:10:17.543414Z","shell.execute_reply":"2025-01-15T18:10:18.394130Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def reconstruction_loss(input_vec, reconstructed_vec):\n    return tf.reduce_mean(tf.square(input_vec - reconstructed_vec))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:18.396053Z","iopub.execute_input":"2025-01-15T18:10:18.396274Z","iopub.status.idle":"2025-01-15T18:10:18.400145Z","shell.execute_reply.started":"2025-01-15T18:10:18.396255Z","shell.execute_reply":"2025-01-15T18:10:18.399429Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def architecture_similarity_loss(input1, input2, latent1, latent2, gmin, gmax, lmax, latent_dim):\n    # Normalize input block vectors to [0, 1]\n    normalized_input1 = (input1 - gmin) / (gmax - gmin)\n    normalized_input2 = (input2 - gmin) / (gmax - gmin)\n\n    # Architecture distance for block vectors\n    d_arch_blocks = tf.reduce_mean(tf.abs(normalized_input1 - normalized_input2), axis=1) / lmax\n\n    # Architecture distance for latent vectors\n    d_arch_latents = tf.reduce_mean(tf.abs(latent1 - latent2), axis=1) / latent_dim\n\n    # Architecture similarity loss\n    # Cast d_arch_blocks to float32\n    return tf.reduce_mean(tf.square(tf.cast(d_arch_blocks, tf.float32) - d_arch_latents))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:18.401156Z","iopub.execute_input":"2025-01-15T18:10:18.401427Z","iopub.status.idle":"2025-01-15T18:10:18.418449Z","shell.execute_reply.started":"2025-01-15T18:10:18.401395Z","shell.execute_reply":"2025-01-15T18:10:18.417636Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def scale_similarity_loss(input1, input2, latent1, latent2, gmin, gmax, lmax, latent_dim):\n    # Normalize input block vectors to [0, 1]\n    normalized_input1 = (input1 - gmin) / (gmax - gmin)\n    normalized_input2 = (input2 - gmin) / (gmax - gmin)\n\n    # Scale distance for block vectors\n    d_scale_blocks = tf.abs(tf.reduce_sum(normalized_input1, axis=1) - tf.reduce_sum(normalized_input2, axis=1))\n\n    # Scale distance for latent vectors\n    d_scale_latents = tf.abs(tf.reduce_sum(latent1, axis=1) - tf.reduce_sum(latent2, axis=1))\n\n    # Scale similarity loss\n    return tf.reduce_mean(tf.square(tf.cast(d_scale_blocks, tf.float32) - d_scale_latents))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:18.419184Z","iopub.execute_input":"2025-01-15T18:10:18.419378Z","iopub.status.idle":"2025-01-15T18:10:18.433209Z","shell.execute_reply.started":"2025-01-15T18:10:18.419362Z","shell.execute_reply":"2025-01-15T18:10:18.432448Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"sample_block_vector = np.random.randint(gmin, gmax + 1, size=(1, lmax))\nlatent_vector = encoder(sample_block_vector)\nreconstructed_block_vector = decoder(latent_vector)\n\nprint(tf.reduce_mean(tf.square(sample_block_vector - reconstructed_block_vector)))\n\nprint(\"Original Block Vector:\", sample_block_vector)\nprint(\"Reconstructed Block Vector:\", reconstructed_block_vector.numpy().astype(int))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:18.434160Z","iopub.execute_input":"2025-01-15T18:10:18.434458Z","iopub.status.idle":"2025-01-15T18:10:19.401015Z","shell.execute_reply.started":"2025-01-15T18:10:18.434428Z","shell.execute_reply":"2025-01-15T18:10:19.400014Z"}},"outputs":[{"name":"stdout","text":"tf.Tensor(40.75, shape=(), dtype=float32)\nOriginal Block Vector: [[11 13 22 29 23 17 20 10 24 22 27 17 31 15 16 22 10 27 18 15]]\nReconstructed Block Vector: [[21 21 22 20 22 22 22 22 21 21 21 21 22 21 21 21 21 22 21 21]]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"optimizer = Adam(learning_rate=0.005)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:19.405167Z","iopub.execute_input":"2025-01-15T18:10:19.405410Z","iopub.status.idle":"2025-01-15T18:10:19.415991Z","shell.execute_reply.started":"2025-01-15T18:10:19.405388Z","shell.execute_reply":"2025-01-15T18:10:19.415013Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"print(n_samples//batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:19.418056Z","iopub.execute_input":"2025-01-15T18:10:19.418343Z","iopub.status.idle":"2025-01-15T18:10:19.428351Z","shell.execute_reply.started":"2025-01-15T18:10:19.418318Z","shell.execute_reply":"2025-01-15T18:10:19.427208Z"}},"outputs":[{"name":"stdout","text":"234\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"epochs=10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:19.429415Z","iopub.execute_input":"2025-01-15T18:10:19.429728Z","iopub.status.idle":"2025-01-15T18:10:19.442390Z","shell.execute_reply.started":"2025-01-15T18:10:19.429691Z","shell.execute_reply":"2025-01-15T18:10:19.441473Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Training loop\nfor epoch in range(10):  # Number of epochs\n    print(f\"Epoch {epoch + 1}\")\n    total_loss = 0\n    total_batches = n_samples // batch_size\n\n    for _ in range(total_batches):\n        # Create pairwise datasets\n        input1, input2 = create_pairwise_datasets(block_vectors, batch_size)\n\n        # Perform a training step\n        with tf.GradientTape() as tape:\n            # Pass the inputs through the autoencoder\n            latent1 = encoder(input1, training=True)\n            latent2 = encoder(input2, training=True)\n            recon1 = decoder(latent1, training=True)\n            recon2 = decoder(latent2, training=True)\n\n            # Calculate custom losses\n            recon_loss_1 = reconstruction_loss(input1, recon1)\n            recon_loss_2 = reconstruction_loss(input2, recon2)\n            arch_sim_loss = architecture_similarity_loss(input1, input2, latent1, latent2, gmin, gmax, lmax, latent_dim)\n            scale_sim_loss = scale_similarity_loss(input1, input2, latent1, latent2, gmin, gmax, lmax, latent_dim)\n\n            # Total loss\n            total_batch_loss = recon_loss_1 + recon_loss_2 + arch_sim_loss + scale_sim_loss\n\n        # Compute gradients and apply them\n        gradients = tape.gradient(total_batch_loss, autoencoder.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, autoencoder.trainable_variables))\n\n        # Accumulate total loss for monitoring\n        total_loss += total_batch_loss.numpy()\n\n    # Average loss for the epoch\n    avg_loss = total_loss / total_batches\n    reconstructed = autoencoder.predict(block_vectors)\n    mse = np.mean(np.square(block_vectors - reconstructed))\n    print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f} , Reconstruction MSE: {mse:.4f}\\n\")\n\nprint(\"Training completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:10:19.443370Z","iopub.execute_input":"2025-01-15T18:10:19.443669Z","iopub.status.idle":"2025-01-15T18:12:32.771856Z","shell.execute_reply.started":"2025-01-15T18:10:19.443634Z","shell.execute_reply":"2025-01-15T18:12:32.771110Z"}},"outputs":[{"name":"stdout","text":"Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:664: UserWarning: Gradients do not exist for variables ['kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\nEpoch 1 completed. Average Loss: 296.9353 , Reconstruction MSE: 145.6962\n\nEpoch 2\n\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\nEpoch 2 completed. Average Loss: 294.5817 , Reconstruction MSE: 145.4779\n\nEpoch 3\n\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\nEpoch 3 completed. Average Loss: 295.9600 , Reconstruction MSE: 146.0742\n\nEpoch 4\n\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\nEpoch 4 completed. Average Loss: 295.9015 , Reconstruction MSE: 145.9635\n\nEpoch 5\n\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\nEpoch 5 completed. Average Loss: 296.2026 , Reconstruction MSE: 146.0551\n\nEpoch 6\n\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\nEpoch 6 completed. Average Loss: 296.0899 , Reconstruction MSE: 145.9487\n\nEpoch 7\n\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\nEpoch 7 completed. Average Loss: 295.6101 , Reconstruction MSE: 145.5424\n\nEpoch 8\n\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\nEpoch 8 completed. Average Loss: 300.2215 , Reconstruction MSE: 145.9238\n\nEpoch 9\n\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\nEpoch 9 completed. Average Loss: 299.9483 , Reconstruction MSE: 146.2645\n\nEpoch 10\n\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\nEpoch 10 completed. Average Loss: 298.5394 , Reconstruction MSE: 146.3417\n\nTraining completed.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"sample_block_vector = np.random.randint(gmin, gmax + 1, size=(1, lmax))\nlatent_vector = encoder(sample_block_vector)\nreconstructed_block_vector = decoder(latent_vector)\n\nprint(tf.reduce_mean(tf.square(sample_block_vector - reconstructed_block_vector)))\n\nprint(\"Original Block Vector:\", sample_block_vector)\nprint(\"Reconstructed Block Vector:\", reconstructed_block_vector.numpy().astype(int))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:32.772795Z","iopub.execute_input":"2025-01-15T18:12:32.773065Z","iopub.status.idle":"2025-01-15T18:12:32.788334Z","shell.execute_reply.started":"2025-01-15T18:12:32.773044Z","shell.execute_reply":"2025-01-15T18:12:32.787440Z"}},"outputs":[{"name":"stdout","text":"tf.Tensor(38.15, shape=(), dtype=float32)\nOriginal Block Vector: [[28 25 14 27 14 18 15 15 12 27 22 15 27 17 23 29 25 29 14 15]]\nReconstructed Block Vector: [[22 21 22 20 22 22 22 22 21 21 21 20 22 21 20 21 21 22 21 22]]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"input1, input2 = create_pairwise_datasets(block_vectors, batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:32.789219Z","iopub.execute_input":"2025-01-15T18:12:32.789460Z","iopub.status.idle":"2025-01-15T18:12:32.795085Z","shell.execute_reply.started":"2025-01-15T18:12:32.789429Z","shell.execute_reply":"2025-01-15T18:12:32.794245Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"input1[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:32.795940Z","iopub.execute_input":"2025-01-15T18:12:32.796218Z","iopub.status.idle":"2025-01-15T18:12:32.809717Z","shell.execute_reply.started":"2025-01-15T18:12:32.796189Z","shell.execute_reply":"2025-01-15T18:12:32.808911Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"array([10, 14, 28, 21, 12, 20, 10, 12, 27, 12, 19, 30, 26,  0,  0,  0,  0,\n        0,  0,  0])"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"input2[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:32.810525Z","iopub.execute_input":"2025-01-15T18:12:32.810762Z","iopub.status.idle":"2025-01-15T18:12:32.823396Z","shell.execute_reply.started":"2025-01-15T18:12:32.810733Z","shell.execute_reply":"2025-01-15T18:12:32.822742Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"array([22, 29, 30, 28, 23, 31, 24, 20, 16, 10, 30,  0,  0,  0,  0,  0,  0,\n        0,  0,  0])"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"latent1 = encoder(input1)\nlatent1[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:32.824170Z","iopub.execute_input":"2025-01-15T18:12:32.824435Z","iopub.status.idle":"2025-01-15T18:12:32.851341Z","shell.execute_reply.started":"2025-01-15T18:12:32.824407Z","shell.execute_reply":"2025-01-15T18:12:32.850701Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(8,), dtype=float32, numpy=array([1., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"print(f\"Shape of x_train: {x_train.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:32.852004Z","iopub.execute_input":"2025-01-15T18:12:32.852193Z","iopub.status.idle":"2025-01-15T18:12:32.856219Z","shell.execute_reply.started":"2025-01-15T18:12:32.852176Z","shell.execute_reply":"2025-01-15T18:12:32.855368Z"}},"outputs":[{"name":"stdout","text":"Shape of x_train: (50000, 32, 32, 3)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"print(block_vectors.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:32.857113Z","iopub.execute_input":"2025-01-15T18:12:32.857350Z","iopub.status.idle":"2025-01-15T18:12:32.869770Z","shell.execute_reply.started":"2025-01-15T18:12:32.857330Z","shell.execute_reply":"2025-01-15T18:12:32.868998Z"}},"outputs":[{"name":"stdout","text":"(15000, 20)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"print(autoencoder.output_shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:32.870739Z","iopub.execute_input":"2025-01-15T18:12:32.871047Z","iopub.status.idle":"2025-01-15T18:12:32.882868Z","shell.execute_reply.started":"2025-01-15T18:12:32.871025Z","shell.execute_reply":"2025-01-15T18:12:32.882009Z"}},"outputs":[{"name":"stdout","text":"(None, 20)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n\n# def plot_relationships(autoencoder, encoder, block_vectors, gmin, gmax):\n#     # Normalize block vectors to [0, 1] for comparison\n#     normalized_block_vectors = (block_vectors - gmin) / (gmax - gmin)\n\n#     # Encode block vectors to latent space\n#     latent_vectors = encoder.predict(normalized_block_vectors)\n\n#     # Compute L1 distances in the latent space\n#     n = len(block_vectors)\n#     latent_distances = []\n#     block_distances = []\n\n#     for i in range(n):\n#         for j in range(i + 1, n):\n#             block_distance = np.sum(np.abs(normalized_block_vectors[i] - normalized_block_vectors[j]))\n#             latent_distance = np.sum(np.abs(latent_vectors[i] - latent_vectors[j]))\n#             block_distances.append(block_distance)\n#             latent_distances.append(latent_distance)\n\n#     # Plot relationship between L1 distances\n#     plt.figure(figsize=(8, 6))\n#     plt.scatter(block_distances, latent_distances, alpha=0.6)\n#     plt.xlabel(\"L1 Distance (Normalized Block Vectors)\")\n#     plt.ylabel(\"L1 Distance (Latent Space)\")\n#     plt.title(\"Relationship Between Block Vector and Latent Space Distances\")\n#     plt.grid(True)\n#     plt.show()\n\n#     # Sum of values in block vectors and latent vectors\n#     block_sums = np.sum(block_vectors, axis=1)\n#     latent_sums = np.sum(latent_vectors, axis=1)\n\n#     # Plot relationship between sum of values\n#     plt.figure(figsize=(8, 6))\n#     plt.scatter(block_sums, latent_sums, alpha=0.6)\n#     plt.xlabel(\"Sum of Block Vector Values\")\n#     plt.ylabel(\"Sum of Latent Vector Values\")\n#     plt.title(\"Relationship Between Block Vector and Latent Vector Sums\")\n#     plt.grid(True)\n#     plt.show()\n\n# # Example usage\n# plot_relationships(autoencoder, encoder, block_vectors, gmin, gmax)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:32.883765Z","iopub.execute_input":"2025-01-15T18:12:32.884039Z","iopub.status.idle":"2025-01-15T18:12:32.895429Z","shell.execute_reply.started":"2025-01-15T18:12:32.884008Z","shell.execute_reply":"2025-01-15T18:12:32.894593Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def build_dense_block(block_vector):\n    block_vector = np.round(block_vector).astype(int)  # Ensure integers\n    inputs = Input(shape=(32, 32, 3))\n    x = inputs\n\n    for growth_rate in block_vector:\n        if growth_rate == 0:\n            continue\n\n        # Convolution-BatchNorm-ReLU sequence\n        conv = Conv2D(growth_rate, kernel_size=(3, 3), strides=1, padding='same')\n        bn = BatchNormalization()\n        act = Activation('relu')\n\n        # Apply sequentially\n        layer = conv(x)\n        layer = bn(layer)\n        layer = act(layer)\n\n        # Concatenate to the block\n        x = Concatenate()([x, layer])\n\n    x = Conv2D(filters=3, kernel_size=1, padding='same', activation='relu')(x)\n    \n    return Model(inputs, x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:32.896282Z","iopub.execute_input":"2025-01-15T18:12:32.896546Z","iopub.status.idle":"2025-01-15T18:12:32.911934Z","shell.execute_reply.started":"2025-01-15T18:12:32.896515Z","shell.execute_reply":"2025-01-15T18:12:32.911129Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def initialize_particles(encoder, n_particles, block_vectors):\n    particles = []\n    for _ in range(n_particles):\n        idx = np.random.randint(0, len(block_vectors))\n        block_vector = block_vectors[idx]\n        latent_vector = encoder.predict(block_vector.reshape(1, -1))\n        particles.append(latent_vector.flatten())\n    return np.array(particles)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:32.912889Z","iopub.execute_input":"2025-01-15T18:12:32.913194Z","iopub.status.idle":"2025-01-15T18:12:32.930126Z","shell.execute_reply.started":"2025-01-15T18:12:32.913165Z","shell.execute_reply":"2025-01-15T18:12:32.929284Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"n_particles = 3\nparticles = initialize_particles(encoder, n_particles, block_vectors)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:32.931017Z","iopub.execute_input":"2025-01-15T18:12:32.931211Z","iopub.status.idle":"2025-01-15T18:12:33.267508Z","shell.execute_reply.started":"2025-01-15T18:12:32.931194Z","shell.execute_reply":"2025-01-15T18:12:33.266827Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"def basic_fitness_evaluation(particles, decoder, x_train, y_train, x_test, y_test, rb, kbmax, lbt):\n    db_train_size = int(rb * len(x_train))\n    db_train = x_train[:db_train_size]\n    db_train_labels = y_train[:db_train_size]\n\n    fitness_scores = []\n    for particle in particles:\n        block_vector = decoder.predict(particle.reshape(1, -1)).flatten()\n        dense_block = build_dense_block(block_vector)\n\n        model = tf.keras.Sequential([\n            dense_block,\n            AveragePooling2D(pool_size=(2, 2)),\n            Flatten(),\n            Dense(num_classes, activation='softmax')\n        ])\n\n        model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n\n        epoch, loss = 0, float('inf')\n        while epoch < kbmax and loss >= lbt:\n            history = model.fit(db_train, db_train_labels, epochs=1, verbose=0)\n            loss = history.history['loss'][-1]\n            epoch += 1\n\n        accuracy = model.evaluate(x_test, y_test, verbose=0)[1]  # Fetch accuracy from the evaluation result\n        fitness_scores.append(accuracy)\n\n    return np.array(fitness_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:33.268410Z","iopub.execute_input":"2025-01-15T18:12:33.268646Z","iopub.status.idle":"2025-01-15T18:12:33.274753Z","shell.execute_reply.started":"2025-01-15T18:12:33.268626Z","shell.execute_reply":"2025-01-15T18:12:33.273866Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def progressive_fitness_evaluation(particles, fitness_scores, decoder, x_train, y_train, x_test, y_test, rprog, kpmax, lpt):\n    progressive_scores = np.zeros(len(particles))\n    top_indices = np.argsort(fitness_scores)[-len(fitness_scores) // 3:]\n\n    for idx in top_indices:\n        particle = particles[idx]\n        block_vector = decoder.predict(particle.reshape(1, -1)).flatten()\n        dense_block = build_dense_block(block_vector)\n\n        for proportion in [0.2, 0.4]:\n            dp_train_size = int(proportion * len(x_train))\n            dp_train = x_train[:dp_train_size]\n            dp_train_labels = y_train[:dp_train_size]\n\n            model = tf.keras.Sequential([\n                dense_block,\n                AveragePooling2D(pool_size=(2, 2)),\n                Flatten(),\n                Dense(num_classes, activation='softmax')\n            ])\n\n            model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), loss='categorical_crossentropy',metrics=['accuracy'])\n\n            epoch, loss = 0, float('inf')\n            while epoch < kpmax and loss >= lpt:\n                history = model.fit(dp_train, dp_train_labels, epochs=1, verbose=0)\n                loss = history.history['loss'][-1]\n                epoch += 1\n\n            accuracy = model.evaluate(x_test, y_test, verbose=0)[1]\n            progressive_scores[idx] = max(progressive_scores[idx], accuracy)\n\n    return progressive_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:33.275587Z","iopub.execute_input":"2025-01-15T18:12:33.275842Z","iopub.status.idle":"2025-01-15T18:12:33.292063Z","shell.execute_reply.started":"2025-01-15T18:12:33.275821Z","shell.execute_reply":"2025-01-15T18:12:33.291285Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def pso_algorithm(particles, decoder, x_train, y_train, x_test, y_test, n_iterations, rb, kbmax, lbt, rprog, kpmax, lpt):\n    velocities = np.zeros_like(particles)\n    personal_best_positions = np.copy(particles)\n    personal_best_scores = basic_fitness_evaluation(particles, decoder, x_train, y_train, x_test, y_test, rb, kbmax, lbt)\n    global_best_position = personal_best_positions[np.argmax(personal_best_scores)]\n    global_best_score = np.max(personal_best_scores)\n\n    w, c1, c2 = 0.7298, 1.49618, 1.49618  # PSO hyperparameters\n    velocity_max = 0.05\n\n    for iteration in range(n_iterations):\n        for i in range(len(particles)):\n            r1, r2 = np.random.rand(), np.random.rand()\n            velocities[i] = w * velocities[i] + c1 * r1 * (personal_best_positions[i] - particles[i]) + c2 * r2 * (global_best_position - particles[i])\n\n            # Clamp velocities\n            velocities[i] = np.clip(velocities[i], -velocity_max, velocity_max)\n\n            # Update particle positions\n            particles[i] += velocities[i]\n\n        fitness_scores = basic_fitness_evaluation(particles, decoder, x_train, y_train, x_test, y_test, rb, kbmax, lbt)\n        progressive_scores = progressive_fitness_evaluation(particles, fitness_scores, decoder, x_train, y_train, x_test, y_test, rprog, kpmax, lpt)\n\n        for i in range(len(particles)):\n            if fitness_scores[i] > personal_best_scores[i]:\n                personal_best_positions[i] = particles[i]\n                personal_best_scores[i] = fitness_scores[i]\n\n        global_best_position = personal_best_positions[np.argmax(personal_best_scores)]\n        global_best_score = np.max(personal_best_scores)\n\n        # Update the global best score and position\n        print(f\"Iteration {iteration + 1}/{n_iterations}, Best Fitness: {global_best_score}\")\n\n        # Early stopping condition: If the global best score doesn't improve for 5 iterations\n        epsilon = 1e-6  # A small tolerance value\n\n        if iteration > 5 and np.abs(np.max(personal_best_scores) - global_best_score) < epsilon:\n              break\n\n    return global_best_position","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:12:33.292870Z","iopub.execute_input":"2025-01-15T18:12:33.293178Z","iopub.status.idle":"2025-01-15T18:12:33.307774Z","shell.execute_reply.started":"2025-01-15T18:12:33.293144Z","shell.execute_reply":"2025-01-15T18:12:33.307059Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# PSO parameters\nn_iterations = 3\nrb = 0.1  # Basic fitness training proportion\nkbmax = 5  # Max epochs for basic fitness\nlbt = 0.5  # Loss threshold for basic fitness\n\nrprog = [0.2, 0.4]  # Progressive training proportions\nkpmax = 5  # Max epochs for progressive fitness\nlpt = 0.5  # Loss threshold for progressive fitness\n\n# Run PSO to find the best dense block\nglobal_best_particle = pso_algorithm(particles, decoder, x_train, y_train, x_test, y_test, n_iterations, rb, kbmax, lbt, rprog, kpmax, lpt)\n\n# Decode the global best particle into a dense block\nfinal_block_vector = decoder.predict(global_best_particle.reshape(1, -1)).flatten()\nfinal_dense_block = build_dense_block(final_block_vector)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:23:51.018095Z","iopub.execute_input":"2025-01-15T18:23:51.018453Z","iopub.status.idle":"2025-01-15T19:02:44.288698Z","shell.execute_reply.started":"2025-01-15T18:23:51.018430Z","shell.execute_reply":"2025-01-15T19:02:44.287606Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\nIteration 1/3, Best Fitness: 0.05829999968409538\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\nIteration 2/3, Best Fitness: 0.05829999968409538\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\nIteration 3/3, Best Fitness: 0.05829999968409538\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"print(\"Output shape of final_dense_block:\", final_dense_block.output_shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T19:08:36.117849Z","iopub.execute_input":"2025-01-15T19:08:36.118306Z","iopub.status.idle":"2025-01-15T19:08:36.123843Z","shell.execute_reply.started":"2025-01-15T19:08:36.118274Z","shell.execute_reply":"2025-01-15T19:08:36.123031Z"}},"outputs":[{"name":"stdout","text":"Output shape of final_dense_block: (None, 32, 32, 3)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"def stack_dense_blocks(block, num_blocks, input_shape):\n    inputs = Input(shape=input_shape)\n    x = inputs\n    for _ in range(num_blocks):\n        x = block(x)\n    x = GlobalAveragePooling2D()(x)  # Pool the feature map to a 1D vector\n    outputs = Dense(100, activation='softmax')(x)  # Match 100 classes\n    return Model(inputs, outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T19:08:38.226477Z","iopub.execute_input":"2025-01-15T19:08:38.226810Z","iopub.status.idle":"2025-01-15T19:08:38.231519Z","shell.execute_reply.started":"2025-01-15T19:08:38.226781Z","shell.execute_reply":"2025-01-15T19:08:38.230647Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"smax = 2\ninput_shape = (32, 32, 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T19:08:44.118561Z","iopub.execute_input":"2025-01-15T19:08:44.118869Z","iopub.status.idle":"2025-01-15T19:08:44.122676Z","shell.execute_reply.started":"2025-01-15T19:08:44.118846Z","shell.execute_reply":"2025-01-15T19:08:44.121843Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"best_model = None\nbest_accuracy = 0\n\nfor num_blocks in range(1, smax + 1):\n    model = stack_dense_blocks(final_dense_block, num_blocks, input_shape)\n    model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), \n                  loss='categorical_crossentropy', \n                  metrics=['accuracy'])\n    \n    model.fit(x_train, y_train, epochs=5, batch_size=64, verbose=1, validation_split=0.2)\n    _, accuracy = model.evaluate(x_test, y_test, verbose=0)\n    \n    if accuracy > best_accuracy:\n        best_accuracy = accuracy\n        best_model = model\n\nprint(f\"Best accuracy achieved: {best_accuracy}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T19:08:45.926402Z","iopub.execute_input":"2025-01-15T19:08:45.926704Z","iopub.status.idle":"2025-01-15T19:35:14.660155Z","shell.execute_reply.started":"2025-01-15T19:08:45.926681Z","shell.execute_reply":"2025-01-15T19:35:14.659069Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 166ms/step - accuracy: 0.0203 - loss: 4.5193 - val_accuracy: 0.0204 - val_loss: 6.1324\nEpoch 2/5\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 159ms/step - accuracy: 0.0467 - loss: 4.1824 - val_accuracy: 0.0202 - val_loss: 6.0363\nEpoch 3/5\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 160ms/step - accuracy: 0.0559 - loss: 4.0498 - val_accuracy: 0.0437 - val_loss: 4.2432\nEpoch 4/5\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 159ms/step - accuracy: 0.0653 - loss: 3.9816 - val_accuracy: 0.0558 - val_loss: 4.0544\nEpoch 5/5\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 159ms/step - accuracy: 0.0723 - loss: 3.9234 - val_accuracy: 0.0675 - val_loss: 4.0018\nEpoch 1/5\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 328ms/step - accuracy: 0.0263 - loss: 4.4188 - val_accuracy: 0.0087 - val_loss: 4.6172\nEpoch 2/5\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 319ms/step - accuracy: 0.0659 - loss: 3.9669 - val_accuracy: 0.0087 - val_loss: 4.6362\nEpoch 3/5\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 320ms/step - accuracy: 0.0734 - loss: 3.8624 - val_accuracy: 0.0102 - val_loss: 4.6575\nEpoch 4/5\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 318ms/step - accuracy: 0.0823 - loss: 3.7843 - val_accuracy: 0.0102 - val_loss: 4.6827\nEpoch 5/5\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 319ms/step - accuracy: 0.0901 - loss: 3.7167 - val_accuracy: 0.0103 - val_loss: 4.7233\nBest accuracy achieved: 0.0658000037074089\n","output_type":"stream"}],"execution_count":45}]}